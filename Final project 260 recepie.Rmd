---
title: Deploying a ML model to predict Fertility outcomes in patient undergoing gamete
  banking A national SART database
author: "Rstudio"
date: "2022-12-01"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

There has been a substantial increase in gamete banking in the last decade, various studies have showed that apart from the scale of this trend, the demographics of women opting for gamete banking has shifted to a younger age group. Success in these cycles is not well defined and the prediction of number of women who eventually returned to use their cryopreserved gametes has been consistently low making an individualized prognosis and counseling using various approaches largely unsuccessful.

## Objective

We used a national cohort analysis of 6873 gamete banking cycles undertaken in the USA 2014--2020, to construct a prediction tool to prognosticate patient success rates. The proposed model could assist patient and clinicians throughout all stages of pre-cycle and cycle in a stepwise approach.

Data of interest: Clinical and embryonic data of patients who underwent gamete banking were used. To model the number of oocytes needed to bank and to predict the estimated storage time, we will used logistic regression (model 1) and a stepwise regression (model 2/primary) as methods for selection. The predictive model will be assessed against a test set. The model discrimination and calibration will be tested by ROC curve. Our final model will output the individualized probability based on the predictors. A web-based calculator will be developed to make two types of predictions automatically: 1. Onco-fertility and PGT-M indications and 2. elective Oocyte freezing cycles. This new banking calculator may assist in clinical counseling and in individualized treatment planning and could better address aspects such as the time number of oocytes and cycles required in order to maximize chances for a live birth.

## Aims and anticipated outcome

Train test and validate a prediction model based on a national SART dataset. This predictive tool, based on multiple predictors and banking indications will assist in an individual counseling and will produce an estimate of the optimum number of oocytes and cycles needed. Moreover, the tool will estimate the probability and time interval for claiming the gametes.

Methods: A retrospective cohort analysis. Clinical data were extracted at from the national SART database <https://www.sart.org/>. We included all women undergoing autologous oocyte or embryo banking procedures, who had their first oocyte retrieval from December 2014 to December 2020 and came back to claim their gametes for intended pregnancy.

Cohort description: Our database includes information contained in electronic charts derived most IVF clinic in the USA, from a female population. Data of interest included patient's demographics, past medical history, and infertility evaluation including diagnosis, laboratory testing for ovarian reserve, and any radiologic studies pertinent to a diagnosis of infertility.

Definition of outcomes: We aimed to predict live birth among those who came to claim their gametes (task 1). The response variable "live birth" is coded 0 for no live birth and 1 for live birth.

## Upload data

```{r, results='hide'}
library(readr)
bank.linked_df <- read.csv("C:\\Users\\Youval Fouks\\Desktop\\Practicum\\CSV\\Linked 2022 SART Data.csv")

```

## Install & load packages

```{r, results='hide', echo=FALSE}
if (!require(Hmisc)) {install.packages("Hmisc")}
if (!require(MASS)) {install.packages("MASS")}
if (!require(caret)) {install.packages("caret")}
if (!require(leaps)) {install.packages("leaps")}
if (!require(gamlr)) {install.packages("gamlr")}
if (!require(glmnet)) {install.packages("glmnet")}
if (!require(sas7bdat)) {install.packages("sas7bdat")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(purrr)) {install.packages("purrr")}
if (!require(pROC)) {install.packages("pROC")}
if (!require(survivalROC)) {install.packages("survivalROC")}
if (!require(survival)) {install.packages("survival")}
if (!require(tidyr)) {install.packages("tidyr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
if (!require(arsenal)) {install.packages("arsenal")}
if (!require(tidyverse)) {install.packages("tidyverse")}
if (!require(mice)) {install.packages("mice")}
library(naniar)
```

```{r, echo=FALSE}
df <- bank.linked_df
```

```{r, echo=FALSE, echo=FALSE}
bank.linked_df$PatientAgeAtStart <- as.numeric(bank.linked_df$PatientAgeAtStart)
```

Preprocessing: Redundancy of the data was treated with variable Selection in order to reach better predictors for the modeling phase. Redundant and highly correlated variables exclusion was made (by clinical judgment).

## Remove unnecessary cols

```{r, echo=FALSE, echo=FALSE}
df <- subset(df, select = -c(5, 6:15, 16:19, 49:52, 56:65, 69, 73:75, 79:81, 87, 88, 93, 100, 101, 103, 104, 106))
```

rename variables

```{r, echo=FALSE}
names(df)[5]=paste("Age")
```

recode var and recode to missing data

```{r}
df[df == "N"] <- "0"
df[df == "Y"] <- "1"
df[df == "No"] <- "0"
df[df == "Yes"] <- "1"

# recode to missing data
df[df=="NaN"] <- NA
df[df=="N/A"] <- NA
df[df=="Not Reported"] <- NA
df[df == "Not Entered"] <- NA
df[df == ""] <- NA
df[df == "NULL"] <- NA
```

Missingness in data
```{r}
naniar::miss_var_summary(df)
df <- df[,colSums(is.na(df))<nrow(df)]
naniar::miss_var_summary(df)
```

Remove splitted cycles
```{r}
df_no_fresh <- df[!(df$RetrievalType_AutologousRetrieval1=="Fresh"),]
df <- df_no_fresh
```

## Descriptive table

```{r}
Table1 <- tableby(PregnancyOutcome_Live.Birth ~ Age + Clinic.Region.USA + Gravidity +  FullTermBirths + PreTermBirths + MaleInfertility + Endometriosis + PolycysticOvaries +  DiminishedOvarianReserve + TubalLigation + Uterine + Unexplained + as.factor(elect) +  as.factor(Onco) +  as.factor(Medical.benign) +    as.factor(Syndromatic.DOR) +  as.factor(DOR..35) ,data=df)
```

```{r}
Table2 <- tableby(C ~ ThawedEmbryo + ThawedOocyte + SpermSource_Partner + SpermSource_Donor + SpermSource_Mixed + TransferAttempted + TreatmentOutcome_Not.Pregnant + TreatmentOutcome_Biochemical + TreatmentOutcome_Clinical.Intrauterine.Gestation + PregnancyOutcome_Outcome.Unknown + PregnancyOutcome_Live.Birth + Pregnancy.Loss.Abortion ,data=df)

##########unknowen and 2PN excluded
#summary(tab1, text=TRUE)
summary(Table1)
summary(Table2)
```

Sum retrival cycles into sum

```{r}

df$Retrieval_1 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval1 == "NA" , 0, 1)
df$Retrieval_2 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval2 == "NA" , 0, 1)
df$Retrieval_3 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval3 == "NA" , 0, 1)
df$Retrieval_4 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval4 == "NA" , 0, 1)
df$Retrieval_5 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval5 == "NA" , 0, 1)
df$Retrieval_6 <-ifelse(df$ThawDateStartDateDiff_AutologousRetrieval6 == "NA" , 0, 1)

df <- df %>%
  mutate(sum_1 = rowSums(across(c(Retrieval_1, Retrieval_2, Retrieval_3, Retrieval_4, Retrieval_5, Retrieval_6)), na.rm=TRUE))
```

```{r, echo=FALSE}
df <- subset(df, select = -c(110:115))
```

Number of retrievals against the results

```{r}
Table3 <- tableby(C ~ as.factor(sum_1) ,data=df)
##########unknowen and 2PN excluded
#summary(tab1, text=TRUE)
summary(Table3)
```

```{r, echo=FALSE}

df$Time_to.claim_1 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval1 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval1)
df$Time_to.claim_2 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval2 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval2)
df$Time_to.claim_3 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval3 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval3)
df$Time_to.claim_4 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval4 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval4)
df$Time_to.claim_5 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval5 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval5)
df$Time_to.claim_6 <- ifelse(df$TransferDateRetrievalDateDiff_AutologousRetrieval6 == "NA" , "NA", df$TransferDateRetrievalDateDiff_AutologousRetrieval6)

df$Time_to.claim_1 <- as.numeric(df$Time_to.claim_1)
df$Time_to.claim_2 <- as.numeric(df$Time_to.claim_2)
df$Time_to.claim_3 <- as.numeric(df$Time_to.claim_3)
df$Time_to.claim_4 <- as.numeric(df$Time_to.claim_4)
df$Time_to.claim_5 <- as.numeric(df$Time_to.claim_5)
df$Time_to.claim_6 <- as.numeric(df$Time_to.claim_6)
```

```{r}
df <- df %>%
  mutate(sum_2 = rowSums(across(c(Time_to.claim_1, Time_to.claim_2, Time_to.claim_3, Time_to.claim_4, Time_to.claim_5, Time_to.claim_6)), na.rm=TRUE))

names(df)[111]=paste("Time_to.claim")
```

```{r, echo=FALSE}
df <- subset(df, select = -c(112:116))
#df <- subset(df, select = -c(112))
```

```{r, echo=FALSE}
names(df)[110]=paste("total_n_fresh_cycles")
names(df)[112]=paste("total_tolast_Thaw")
```

Trim all in between cycles after aggregating the data

```{r, echo=FALSE}
df.big.trim <- subset(df, select = -c(58:106))
#df <- subset(df, select = -c(112))
```

```{r, echo=FALSE}
naniar::miss_var_summary(df.big.trim)
```

## Missing values

I have been faced with missing data of various extent. After removing structural missingness, we have summed our missing percentages that in some cases has reached 11%.

Treatment for missing data: First: for each attribute, I test for the correlation of missingness with the outcome and explore with the clinician (my boss) whether or not missingness could potentially encode for a certain prognosis. After solving all structural missingness, I have checked for relations between target and potentially imputed variables (scatter kNN). Using kNN-imputation (k=20) for treating missing data and to control noise in the data by assigning values to miss data based on the closest class or cluster that does not have a missing value.

Treat structural missingness

```{r}
df.big.trim$FullTermBirths <- ifelse(df.big.trim$Gravidity == 0 , 0, df.big.trim$FullTermBirths)
df.big.trim$PreTermBirths <- ifelse(df.big.trim$Gravidity == 0 , 0, df.big.trim$PreTermBirths)
df.big.trim$BiochemicalPregnancies <- ifelse(df.big.trim$Gravidity == 0 , 0, df.big.trim$BiochemicalPregnancies)
df.big.trim$SpontaneousAbortions <- ifelse(df.big.trim$Gravidity == 0 , 0, df.big.trim$SpontaneousAbortions)

df.big.trim$NumberBorn[is.na(df.big.trim$NumberBorn)] <- 0
df.big.trim$NumberLiveBorn[is.na(df.big.trim$NumberLiveBorn)] <- 0

df.big.trim$Unknowen[is.na(df.big.trim$Unknowen)] <- 0
df.big.trim$elect[is.na(df.big.trim$elect)] <- 0
df.big.trim$Onco[is.na(df.big.trim$Onco)] <- 0
df.big.trim$Medical.benign[is.na(df.big.trim$Medical.benign)] <- 0
df.big.trim$Medical.Tumor.prevention[is.na(df.big.trim$Medical.Tumor.prevention)] <- 0
df.big.trim$Syndromatic.DOR[is.na(df.big.trim$Syndromatic.DOR)] <- 0
df.big.trim$DOR..35[is.na(df.big.trim$DOR..35)] <- 0
```

```{r, echo=FALSE}

df.big.trim <- subset(df.big.trim, select = -c(9, 24, 38, 44, 45, 51, 54, 57,59))
```

```{r, echo=FALSE}
df <-df.big.trim

df$TransferAttempted <- as.factor(df$TransferAttempted)
df$Clinic.Region.USA <- as.factor(df$Clinic.Region.USA)
df$Age <- as.numeric(df$Age)
df$RetrievalType_AutologousRetrieval1.1 <- as.numeric(df$RetrievalType_AutologousRetrieval1.1)
df$NumberBorn <- as.numeric(df$NumberBorn)
df$DonorIdentityKnown  <- as.factor(df$DonorIdentityKnown )
df$Total2PN <- as.numeric(df$Total2PN)

cols <- names(df)[7:10] # or column index (change the index if needed)
df[cols] <- lapply(df[cols], as.integer)
cols1 <- names(df)[46:49] # or column index (change the index if needed)
df[cols1] <- lapply(df[cols1], as.numeric)
cols2 <- names(df)[1:4] # or column index (change the index if needed)
df[cols2] <- lapply(df[cols2], as.factor)
cols3 <- names(df)[11:35] # or column index (change the index if needed)
df[cols3] <- lapply(df[cols3], as.factor)
cols4 <- names(df)[38:45] # or column index (change the index if needed)
df[cols4] <- lapply(df[cols4], as.factor)

#trim missing
trim <- c("RetrievalType_AutologousRetrieval1.1", "Total2PN","StartDateFreezeDateDiff_AutologousRetrieval1_Freeze1_1_Autologous_1", "BiochemicalPregnancies", "SpontaneousAbortions")
df <- df[, !(names(df) %in% trim)]

```

## kNN Imputation

\
kNN Imputation for Missing Values Relations between target and imputed

```{r}

ggplot(df, aes(x = Time_to.claim , y = NumberRetrieved_AutologousRetrieval1, color = Age)) + 
  geom_point(show.legend = TRUE) +
  labs(x = 'Time_to.claim', y='NumberRetrieved_AutologousRetrieval1',  title = "plot for KNN impute",
       color = 'Age') + 
  scale_color_gradient(low = "green", high = "red",
                       na.value = "blue", guide = "legend") +
  theme_minimal()+theme(legend.position="bottom")

ggplot(df, aes(x = Gravidity , y = NumberRetrieved_AutologousRetrieval1, color = Age)) + 
  geom_point(show.legend = TRUE) +
  labs(x = 'Gravidity', y='NumberRetrieved_AutologousRetrieval1',  title = "plot for KNN impute",
       color = 'Age') + 
  scale_color_gradient(low = "green", high = "red",
                       na.value = "blue", guide = "legend") +
  theme_minimal()+theme(legend.position="bottom")

ggplot(df, aes(x = FullTermBirths  , y = NumberRetrieved_AutologousRetrieval1, color = Age)) + 
  geom_point(show.legend = TRUE) +
  labs(x = 'FullTermBirths ', y='NumberRetrieved_AutologousRetrieval1',  title = "plot for KNN impute",
       color = 'Age') + 
  scale_color_gradient(low = "green", high = "red",
                       na.value = "blue", guide = "legend") +
  theme_minimal()+theme(legend.position="bottom")
```

```{r}

library(caret)
library(RANN)
preProcValues <- preProcess(df %>% 
                          dplyr::select(Time_to.claim, Gravidity, FullTermBirths ,Age,  DonorIdentityKnown, FullTermBirths,  MaleInfertility , elect, DOR..35, SpermSource_Partner, SpermSource_Donor, NumberRetrieved_AutologousRetrieval1, ThawDateStartDateDiff_AutologousRetrieval1 ),
                            method = c("knnImpute"),
                            k = 20,
                            knnSummary = mean)
impute_df_info <- predict(preProcValues, df,na.action = na.pass)

procNames <- data.frame(col = names(preProcValues$mean), mean = preProcValues$mean, sd = preProcValues$std)
for(i in procNames$col){
 impute_df_info[i] <- impute_df_info[i]*preProcValues$std[i]+preProcValues$mean[i] 
}
```

```{r}
df <- impute_df_info
pct_miss(df)
```

## Data inspection

```{r, warning=FALSE}
library(PerformanceAnalytics)
df.numeric <- df %>% dplyr::select(where(is.numeric))
chart.Correlation(df.numeric, histogram=TRUE, pch="+")
```

Model selection: I used logistic regressions as a bassline comparative fit to model Class as a function of my predictors. The first method I used was "all variable included" logistic regression. Then used a stepwise selection process to narrow down the predictors from the initial regression to interpret which factors are the most important to accurately classifying live birth. The predictive ML models were assessed on the basis of the objectives: Penelaized regressions (lasso ridge), Random Forest, and SVM for predicting IVF outcome of live birth.

I have splitted 75% of the data for training using our new factorized variable and the remaining 25% for testing.

Data Partition

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df))

## set the seed 
set.seed(2050)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train <- df[train_ind, ]
test <- df[-train_ind, ]
```

## Automated regression and logistic regression

stepwise variable selection process to narrow down the predictors in the regression to interpret which factors are the most important to accurately classifying the response variable.

The estimates from logistic regression characterize the relationship between the predictors and response variable on a log-odds scale.

```{r}
#build models
logreg <- glm(PregnancyOutcome_Live.Birth ~ Clinic.Region.USA + Age +  DonorIdentityKnown +  Gravidity +  FullTermBirths +   MaleInfertility + Endometriosis + PolycysticOvaries + DiminishedOvarianReserve + TubalLigation + TubalHydrosalpinx + 
TubalOther + Uterine + Unexplained + OtherNonInfertile +  OtherPGD + Unknowen + elect + DOR..35 + ThawedEmbryo + ThawedOocyte + SpermSource_Partner + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  ThawDateStartDateDiff_AutologousRetrieval1 + Time_to.claim, data = train, family = 'binomial')

logreg_steps <- glm(PregnancyOutcome_Live.Birth ~ Clinic.Region.USA + Age +  DonorIdentityKnown +  Gravidity +  FullTermBirths +   MaleInfertility + Endometriosis + PolycysticOvaries + DiminishedOvarianReserve + TubalLigation + TubalHydrosalpinx + 
TubalOther + Uterine + Unexplained + OtherNonInfertile +  OtherPGD + Unknowen + elect + DOR..35 + ThawedEmbryo + ThawedOocyte + SpermSource_Partner + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  ThawDateStartDateDiff_AutologousRetrieval1 + Time_to.claim, data = train, family = 'binomial') %>% stepAIC(trace = F)
step.model <- stepAIC(logreg, trace = FALSE)

#prediction on test data
log.predict <- predict(logreg, newdata = test, type = 'response')
## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == 
step.predict <- predict(logreg_steps, newdata = test, type = 'response')

```

After training and testing the model, it's important to understand how well that model did in regards to its accuracy and predictive power. Our models accurately predicted 65% of the observations in the testing set.

## Logistic Regression

```{r}
#confusion matrix
log.prediction.rd <- ifelse(log.predict > 0.5, 1, 0)
step.prediction <- ifelse(step.predict > .5, 1, 0)

table_2 <- table(log.prediction.rd, test$PregnancyOutcome_Live.Birth)
table_3 <- table(step.prediction, test$PregnancyOutcome_Live.Birth)

Accuracy_logreg <- sum(diag(table_2))/(sum(table_2))
Accuracy_stepwise <- sum(diag(table_3))/(sum(table_3))

labels <- c("Logisitic Regression", "LogReg with Stepwise")
values <- c(Accuracy_logreg, Accuracy_stepwise)

Accuracy_table <- data.frame("Model" = labels, "Accuracy Rate" = values)
Accuracy_table
```

Prediction & Confusion Matrix -- test data

```{r}
step.predict <- predict(logreg_steps, newdata = test, type = 'response')
```

A fitted model of the data in which to do predictions now we will check our fit against testing dataset.

summarizing and visualizing regression models

```{r}
library(jtools)
summ(step.model)
```

# 

## 2 Random Forest 

Data Partition

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df))

## set the seed 
set.seed(2050)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train.x <- df[train_ind, ]
test.x <- df[-train_ind, ]
```

```{r}

rf1 <- train(PregnancyOutcome_Live.Birth ~ Age + Gravidity +  FullTermBirths + Clinic.Region.USA +  MaleInfertility + Endometriosis  + Uterine + Unexplained +  OtherPGD + Unknowen + elect + Onco + OtherRFA + Medical.benign + Medical.Tumor.prevention + Syndromatic.DOR + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  Time_to.claim + total_n_fresh_cycles, 
                    data= train.x ,  
                    method ="rf",
                    TuneLength = 5,
                    proximity=TRUE, 
                    trControl = trainControl(method = "cv", number=12 ))
# OtherNonInfertile + ThawedEmbryo + TransferAttempted + DonorIdentityKnown  + OtherRFA + SpermSource_Partner
print(rf1)
```

```{r}
rf1$results
#p.rf <- predict(rf, train.x)
#confusionMatrix(p.rf, train.x$PregnancyOutcome_Live.Birth)
```

```{r}
library(randomForest)
rf <- randomForest(PregnancyOutcome_Live.Birth ~Age + Gravidity +  FullTermBirths + Clinic.Region.USA +  MaleInfertility + Endometriosis  + Uterine + Unexplained +  OtherPGD + Unknowen + elect + Onco + OtherRFA + Medical.benign + Medical.Tumor.prevention + Syndromatic.DOR + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  Time_to.claim + total_n_fresh_cycles, 
                    data= train.x ,
                   TuneLength = 5,
                   importance = T)
# OtherNonInfertile + ThawedEmbryo + TransferAttempted + DonorIdentityKnown  + OtherRFA + SpermSource_Partner

```

Prediction & Confusion Matrix -- train data:

```{r}
p.rf <- predict(rf, train.x)
confusionMatrix(p.rf, train.x$PregnancyOutcome_Live.Birth)
```

Prediction & Confusion Matrix -- test data

```{r}
p2 <- predict(rf1, test.x)
confusionMatrix(p2, test.x$PregnancyOutcome_Live.Birth)
```

Variable Importance: These are the variables that the algorithm detected as the most important.

Error rate of Random Forest

```{r}
plot(rf1)
randomForest::importance(rf)
#Variable Importance
varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
importance(rf)

hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "green")
#Variable Importance
varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")

plot(rf)
```

Tune my RF

```{r}

a=c()
i=7
for (i in 2:10) {

rf.tune <- randomForest(PregnancyOutcome_Live.Birth ~ Age + Gravidity +  FullTermBirths + Clinic.Region.USA +  MaleInfertility + Endometriosis  + Uterine + Unexplained +  OtherPGD + Unknowen + elect + Onco + OtherRFA + Medical.benign + Medical.Tumor.prevention + Syndromatic.DOR + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  Time_to.claim + total_n_fresh_cycles, 
                    data= train.x ,
                    ntree= 400,
                    mtry=i, 
                    importance= TRUE,
                    proximity=TRUE, 
                    na.action=na.roughfix)

preValid <- predict(rf.tune, test.x, type = "class")
a[i-2] = mean(preValid == test.x$PregnancyOutcome_Live.Birth)
}
a
plot(3:10, a)
```

Final Model

```{r}

rf.f <- randomForest(PregnancyOutcome_Live.Birth ~ Age + Gravidity +  FullTermBirths + Clinic.Region.USA +  MaleInfertility + Endometriosis  + Uterine + Unexplained +  OtherPGD + Unknowen + elect + Onco + OtherRFA + Medical.benign + Medical.Tumor.prevention + Syndromatic.DOR + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  Time_to.claim + total_n_fresh_cycles, 
                    data= train.x , 
                    ntree= 400,
                    mtry=2, 
                    proximity=TRUE, 
                    na.action=na.roughfix,
                    trControl = trainControl(method = "cv", number=23 ))
# OtherNonInfertile + ThawedEmbryo + TransferAttempted + DonorIdentityKnown  + OtherRFA + SpermSource_Partner

```

```{r}
print(rf.f)
```

Prediction & Confusion Matrix -- test data

```{r}
p.final.t <- predict(rf.f, train.x)
confusionMatrix(p.final.t, train.x$PregnancyOutcome_Live.Birth)

p.final <- predict(rf.f, test.x)
confusionMatrix(p.final, test.x$PregnancyOutcome_Live.Birth)
```

```{r}
treeT1 <- table(predict(rf.f, test.x, type = 'class'), test.x$PregnancyOutcome_Live.Birth)
Accuracyrf <- (treeT1[1,1]+treeT1[2,2])/(sum(treeT1))
Accuracyrf
```

# 

## 3 Lasso

```{r}
# Install & load packages
if (!require(sas7bdat)) {install.packages("sas7bdat")}
if (!require(Hmisc)) {install.packages("Hmisc")}
if (!require(MASS)) {install.packages("MASS")}
if (!require(caret)) {install.packages("caret")}
if (!require(leaps)) {install.packages("leaps")}
if (!require(gamlr)) {install.packages("gamlr")}
if (!require(glmnet)) {install.packages("glmnet")}
```

Trim non factors before "hot-encoding"

```{r}
#trim missing
trim <- c("ExternalPatientID ", "ExternalCycleId ")
df <- df[, !(names(df) %in% trim)]

df <- subset(df, select = -c(3,4, 35:38, 40:41 ,45, 46, 49)) 
```

Data Partition

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df))
## set the seed 
set.seed(1732)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train.lasso <- df[train_ind, ]
test.lasso <- df[-train_ind, ]
```

Create the matrix of predictors and also automatically converts categorical predictors to dummy variables

```{r}
# Dumy code categorical predictor variables
x <- model.matrix(PregnancyOutcome_Live.Birth ~., train.lasso)[,-1]
y <- train.lasso$PregnancyOutcome_Live.Birth
```

Fit the lasso penalized regression model:

```{r}
# Find the best lambda using cross-validation
#Find the optimal value of lambda that minimizes the cross-validation error
set.seed(123) 
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
plot(cv.lasso)
# Display regression coefficients
coef(model)
```

```{r}
# Make predictions on the test data
x.test <- model.matrix(PregnancyOutcome_Live.Birth ~., test.lasso)[,-1]
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
# Model accuracy
observed.classes <- test.lasso$PregnancyOutcome_Live.Birth
mean(predicted.classes == observed.classes)
```

```{r}
cv.lasso$lambda.min
cv.lasso$lambda.1se
coef(cv.lasso, cv.lasso$lambda.min)
```

Compute the final lasso modelusing lambda.min:

```{r}
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(PregnancyOutcome_Live.Birth ~., test.lasso)[,-1]
probabilities.lasso <- lasso.model %>% predict(newx = x.test)
predicted.classes.lasso <- ifelse(probabilities > 0.5, "1", "0")
# Model accuracy
observed.classes.lasso <- test.lasso$PregnancyOutcome_Live.Birth
Accuracy_lasso <- mean(predicted.classes.lasso == observed.classes.lasso)
```

Compute the full logistic model

```{r}
# Fit the model
log.model <- glm(PregnancyOutcome_Live.Birth ~., data = train.lasso, family = binomial)
# Make predictions
probabilities.reg <- log.model %>% predict(test.lasso, type = "response")
predicted.classes.reg <- ifelse(probabilities > 0.5, "1", "0")
# Model accuracy
observed.classes.reg <- test.lasso$PregnancyOutcome_Live.Birth
mean(predicted.classes.reg == observed.classes.reg)

```

# 

## 4 SVM

Data Partition

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(df))
## set the seed 
set.seed(1732)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train <- df[train_ind, ]
test <- df[-train_ind, ]
```

Option 1 (as dataset not Matrix)

```{r}
library(caTools)
library(e1071)
#build model
tuning.para <- tune.svm(PregnancyOutcome_Live.Birth~., 
                        data = train, gamma = 10^-2, 
                        cost = 10, tunecontrol = tune.control(cross=10))
svm.model <- tuning.para$best.model
```

```{r}
#predict model
svm.pred <- predict(svm.model, test)
svm.pred <- as.data.frame(svm.pred)
```

```{r}
svm.table <- table(svm.pred$svm.pred, test$PregnancyOutcome_Live.Birth)
Accuracy_svm <- sum(diag(svm.table))/(sum(svm.table))
Accuracy_svm
```

## Models Discrimination

We wanted to assess the classifier performance. Using the proportion of positive data points that are correctly considered as positive and the proportion of negative data points that are mistakenly considered as positive. The area under the ROC curve is 0.660 and 0.662 indicate that the model is not very efficient in discriminating between live birth and no live birth outcome.

Model Discrimination ROC for logreg

```{r}
test$p_e <- predict(step.model, type="response", newdata=test)
roccurve.stepwise <- roc(test$PregnancyOutcome_Live.Birth ~ as.numeric(test$p_e))
auc_stepwise <- roccurve.stepwise # 
```

```{r}
test$p_reg <- predict(log.model, type="response", newdata=test)
roccurve.reg <- roc(test$PregnancyOutcome_Live.Birth ~ as.numeric(test$p_reg))
auc_reg <- roccurve.reg #
```

```{r}
train$p_a <- predict(step.model, type="response", newdata=train)
test$p_a <- predict(step.model, type="response", newdata=test)

roccurve.stepwise.tr <- roc(train$PregnancyOutcome_Live.Birth ~ train$p_a); roccurve.stepwise.tr # 
plot(roccurve.stepwise.tr, legacy.axes=T, main="ROC curve for stepwise Model", col="blue")

roccurve.stepwise.ts <- roc(test$PregnancyOutcome_Live.Birth ~ test$p_a); roccurve.stepwise.ts # 
plot(roccurve.stepwise.ts, legacy.axes=T, main="ROC curve for stepwise Model", col="red")
# Compare AUCs using DeLong's test
roc.test(roccurve.stepwise.tr, roccurve.stepwise.ts, alternative="two.sided") 
# ROC curves of the two models 
plot(roccurve.stepwise.tr, legacy.axes=T, col="blue"); plot(roccurve.stepwise.ts, legacy.axes=T, col="red", add=T) 
legend("bottomright", legend=c("AUC (Model A): 0.72", "AUC (Model B): 0.68"), col=c("blue", "red"), lty=1:1, cex=0.3)

```

Discrimination for random forest

```{r}
rf_prediction <- predict(rf, test.x, type ="prob")
# build the logistic regression model and test it
lr_model <- glm(PregnancyOutcome_Live.Birth ~ Age + Gravidity +  FullTermBirths + Clinic.Region.USA +  MaleInfertility + Endometriosis  + Uterine + Unexplained +  OtherPGD + Unknowen + elect + Onco + OtherRFA + Medical.benign + Medical.Tumor.prevention + Syndromatic.DOR + SpermSource_Donor + NumberRetrieved_AutologousRetrieval1 +  Time_to.claim + total_n_fresh_cycles, data = train.x, family = "binomial")
lr_prediction <- predict(lr_model, test.x, type = "response")

# ROC curves
ROC_rf <- roc(test.x$PregnancyOutcome_Live.Birth, rf_prediction[,2])
ROC_lr <- roc(test.x$PregnancyOutcome_Live.Birth, lr_prediction)

# Area Under Curve (AUC) for each ROC curve (higher -> better)
ROC_rf_auc <- auc(ROC_rf)
ROC_lr_auc <- auc(ROC_lr)
```

Discrimination

```{r}
plot(ROC_rf, col = "red", main = "ROC For Random Forest (RED) vs Logistic Regression (BLUE)")
lines(ROC_lr, col = "blue")
# print the performance of each model
paste("Accuracy % of random forest: ", mean(test.x$PregnancyOutcome_Live.Birth == round(rf_prediction[,2], digits = 0)))
paste("Accuracy % of logistic regression: ", mean(test.x$PregnancyOutcome_Live.Birth == round(lr_prediction, digits = 0)))
paste("Area under curve of random forest: ", ROC_rf_auc)
paste("Area under curve of logistic regression: ", ROC_lr_auc)
```

Discrimination for Lasso

A list of cross-validated ROC data, one for each model along the path.The first line identifies the lambda value giving the best area under the curve (AUC). Then we plot all the ROC curves in grey and the "winner" in red.

```{r}
auc_lasso <- auc(test.lasso$PregnancyOutcome_Live.Birth, probabilities.lasso)
```

```{r}
cfit <- cv.glmnet(x, y, family = "binomial", type.measure = "auc", 
                  keep = TRUE)
rocs <- roc.glmnet(cfit$fit.preval, newy = y)

best <- cv.lasso$index["min",]
plot(rocs[[best]], type = "l")
invisible(sapply(rocs, lines, col="grey"))
lines(rocs[[best]], lwd = 2,col = "red")
```

Discrimination for SVM

```{r}
test$p_e <- predict(svm.model, type="response", newdata=test)
roccurve.st.ts <- roc(test$PregnancyOutcome_Live.Birth ~ as.numeric(test$p_e))
auc_svm <- roccurve.st.ts # 
```

```{r}
train$p_e <- predict(svm.model, type="response", newdata=train)
test$p_e <- predict(svm.model, type="response", newdata=test)

roccurve.st.tr <- roc(train$PregnancyOutcome_Live.Birth ~ as.numeric(train$p_e)); roccurve.st.tr # 
plot(roccurve.st.tr, legacy.axes=T, main="ROC curve for SVM Model", col="blue")

roccurve.st.ts <- roc(test$PregnancyOutcome_Live.Birth ~ as.numeric(test$p_e)); roccurve.st.ts # 
plot(roccurve.st.ts, legacy.axes=T, main="ROC curve for SVM Model", col="red")
# Compare AUCs using DeLong's test
roc.test(roccurve.st.tr, roccurve.st.ts, alternative="two.sided") 
# ROC curves of the two models 
plot(roccurve.st.tr, legacy.axes=T, col="blue"); plot(roccurve.st.ts, legacy.axes=T, col="red", add=T) 
legend("bottomright", legend=c("AUC (Model A): 0.66", "AUC (Model B):  0.66"), col=c("blue", "red"), lty=1:1, cex=0.3)
```

```{r}
AUC_logreg <- auc_reg$auc[1]
AUC_stepwise <-auc_stepwise$auc[1]
AUC_lasso <- auc_lasso[1]
AUC_rf <- auc(ROC_rf)[1]
AUC_svm <- auc_svm$auc[1]
```

For the project construct a comparative table with all the accuracy

```{r}
labels.accuracy <- c("Logisitic Regression", "LogReg with Stepwise", "lasso", "Random Forest", "SVM")
values.accuracy <- c(Accuracy_logreg, Accuracy_stepwise, Accuracy_lasso, Accuracyrf, Accuracy_svm)
accuracytable <- data.frame("Model" = labels.accuracy, "Accuracy Rate" = values.accuracy)

labels.auc <- c("Logisitic Regression", "LogReg with Stepwise", "lasso", "Random Forest", "SVM")
values.auc <- c(AUC_logreg, AUC_stepwise, AUC_lasso, AUC_rf, AUC_svm)
auctable <- data.frame("Model" = labels.accuracy, "AUC" = values.auc )
auctable <- as.data.frame(auctable)

SUM <- merge(accuracytable, auctable, by= "Model")
SUM
```

## Conclusions

Prediction of IVF success is a very hard task!! ALL MODELS preform pretty bad.
Based on the classification error rate, the step-wise logistic regression has the highest classification accuracy rate of 0.65. However, apart of SVM all of the models have very similar performance metrics so any of them would be appropriate to use.

Using ML models on our predictive ability did not improved compered to basic regression model . Many factors could account for this low performance including: quality of the data, the types of the modeling technique, and noisy data. We cannot account for other unknown omitted variables. Therefore, one should consider all of these factors when looking at the classification rate and determining whether it's 'good enough. I should consider revising the individual predictors that are in the model and consider if any other explanatory variables should be included.

